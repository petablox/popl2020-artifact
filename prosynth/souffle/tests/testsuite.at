# Souffle - A Datalog Compiler
# Copyright (c) 2013, 2015, 2016 Oracle and/or its affiliates. All rights reserved.
# Licensed under the Universal Permissive License v 1.0 as shown at:
# - https://opensource.org/licenses/UPL
# - <souffle root>/licenses/SOUFFLE-UPL.txt

AT_INIT([Souffle])
AT_COPYRIGHT([Copyright (c) 2013-15, Oracle and/or its affiliates.])

AT_COLOR_TESTS

dnl test failure with message
dnl $1 -- message to output
m4_define([FAIL],[AT_CHECK([echo "$1" >> testsuite.log ; exit 99])])

dnl Check if a file exists
dnl $1 -- file name to test
m4_define([FILE_EXISTS],[
  AS_IF([test -f "$1"],[],[FAIL("$1 does not exist")])
])

dnl Check whether the contents of two files are identical
dnl $1 -- first file
dnl $2 -- second file
m4_define([SAME_FILE],[
  FILE_EXISTS(["$1"])
  FILE_EXISTS(["$2"])
  AT_CHECK([diff "$1" "$2"],[],[],[])
])

dnl Check whether two files have identical sorted contents.
dnl the contents of two files are identical
dnl $1 -- generated file
dnl $2 -- expected file
m4_define([SORTED_SAME_FILE],[
    sort "$1" > "$(basename $1).sorted.generated"
    sort "$2" > "$(basename $2).sorted.expected"
    SAME_FILE(["$(basename $1).sorted.generated"], ["$(basename $2).sorted.expected"])
])

dnl Check whether all files matching a regex have identical sorted contents
dnl to those in the given directory.
dnl the contents of two files are identical
dnl $1 -- input files regex
dnl $2 -- directory containing the expected output files
m4_define([SORTED_SAME_FILES],[
  for i in $1
  do
    sort "$i" > "$i.sorted.generated"
    sort $2/"$i" > "$i.sorted.expected"
    SAME_FILE(["$i.sorted.generated"], ["$i.sorted.expected"])
  done
])

dnl Check whether two files have the same number of lines
dnl $1 -- first file
dnl $2 -- second file
m4_define([SAME_LINE_COUNT],[
  FILE_EXISTS([$1])
  FILE_EXISTS([$2])
  m4_define([COUNT_F1],["F1.count"])
  m4_define([COUNT_F2],["F2.count"])
  cat "$1" | wc -l > COUNT_F1
  cat "$2" | wc -l > COUNT_F2
  AT_CHECK([diff COUNT_F1 COUNT_F2],[],[],[])
])

dnl Group test for all flag configurations
dnl $1 -- directory of testcase
dnl $2 -- test category
dnl $3 -- command to execute testcase
m4_define([TEST_GROUP],[
  m4_foreach([FLAGS],[CONFS],[
    AT_SETUP([$1 FLAGS])
    $2
    AT_CLEANUP([])
  ])
])

dnl Execute a positive test case for a given flag configuration
dnl $1 -- test case
dnl $2 -- category
dnl $3 -- facts directory relative to the test directory
dnl $4 -- directory with expected output
dnl       (relative to the test dir, but starting with '/'), or empty string
m4_define([TEST_EVAL],[
  m4_define([TESTNAME],[$1])
  m4_define([CATEGORY],[$2])
  m4_define([TESTDIR],["$TESTS"/CATEGORY/TESTNAME])
  m4_define([PROGRAM],[TESTDIR/TESTNAME.dl])
  m4_define([FACTS],[TESTDIR/$3])
  m4_define([EXPECTEDDIR], [TESTDIR$4])
  # invoke souffle
  AT_CHECK(["$SOUFFLE" FLAGS -D. -F FACTS PROGRAM 1>TESTNAME.out 2>TESTNAME.err], [0])
  SORTED_SAME_FILES([*.csv],[EXPECTEDDIR])
  # validate whether the number of generated CSV files
  # is equal to the number of expected CSV files.
  ls *.csv|wc -l >"num.generated"
  ls EXPECTEDDIR/*.csv|wc -l >"num.expected"

  # remove mpi messages
  cat TESTNAME.out | \
  tr '\n' '\r' | \
  perl -pi -e 's/--*\x0D.*\x0D--*\x0D//g' | \
  tr '\r' '\n' > TESTNAME.nompi.out

  # sort output files as printsize statements are non-deterministically ordered
  SORTED_SAME_FILE([TESTNAME.nompi.out],[EXPECTEDDIR/TESTNAME.out])
  SAME_FILE([TESTNAME.err],[EXPECTEDDIR/TESTNAME.err])
  SAME_FILE([num.generated],[num.expected])
])

dnl Execute a positive test case for a given flag configuration
dnl $1 -- test case
dnl $2 -- category
dnl $3 -- facts directory relative to the test directory
dnl $4 -- directory with expected output
dnl       (relative to the test dir, but starting with '/'), or empty string
m4_define([TEST_EVAL_IN],[
  m4_define([TESTNAME],[$1])
  m4_define([CATEGORY],[$2])
  m4_define([TESTDIR],["$TESTS"/CATEGORY/TESTNAME])
  m4_define([PROGRAM],[TESTDIR/TESTNAME.dl])
  m4_define([FACTS],[TESTDIR/$3])
  m4_define([EXPECTEDDIR], [TESTDIR$4])
  # invoke souffle
  AT_CHECK(["$SOUFFLE" FLAGS -D. -F FACTS PROGRAM 0<TESTDIR/TESTNAME.in 1>TESTNAME.out 2>TESTNAME.err], [0])
  SORTED_SAME_FILES([*.csv],[EXPECTEDDIR])
  # validate whether the number of generated CSV files
  # is equal to the number of expected CSV files.
  ls *.csv|wc -l >"num.generated"
  ls EXPECTEDDIR/*.csv|wc -l >"num.expected"

  # remove non-deterministic record numbering and mpi error messages
  cat TESTNAME.out | \
  sed 's/subproof \(@<:@a-zA-Z0-9?_@:>@*\)(@<:@0-9@:>@*)/subproof \1()/g' | \
  sed -e '$a\' | \
  tr '\n' '\r' | \
  perl -pi -e 's/--*\x0D.*\x0D--*\x0D//g' | \
  tr '\r' '\n' > TESTNAME.nompi.out

  # sort output files as printsize statements are non-deterministically ordered
  SORTED_SAME_FILE([TESTNAME.nompi.out],[EXPECTEDDIR/TESTNAME.out])
  SAME_FILE([TESTNAME.err],[EXPECTEDDIR/TESTNAME.err])
  SAME_FILE([num.generated],[num.expected])
])

dnl Execute a negative test case for a given flag configuration
dnl $1 -- test case
dnl $2 -- category
m4_define([TEST_EVAL_ERROR],[
  m4_define([TESTNAME],[$1])
  m4_define([CATEGORY],[$2])
  m4_define([TESTDIR],["$TESTS"/CATEGORY/TESTNAME])
  m4_define([PROGRAM],[TESTDIR/TESTNAME.dl])
  m4_define([FACTS],[TESTDIR/facts])
  AT_CHECK(["$SOUFFLE" FLAGS -D. -F FACTS PROGRAM 1>TESTNAME.out 2>TESTNAME.err], [1])
  SAME_FILE([TESTNAME.out],[TESTDIR/TESTNAME.out])
  SAME_FILE([TESTNAME.err],[TESTDIR/TESTNAME.err])
])

dnl Execute a negative test case for a given flag configuration
dnl $1 -- test case
dnl $2 -- category
m4_define([TEST_EVAL_ERROR_IN],[
  m4_define([TESTNAME],[$1])
  m4_define([CATEGORY],[$2])
  m4_define([TESTDIR],["$TESTS"/CATEGORY/TESTNAME])
  m4_define([PROGRAM],[TESTDIR/TESTNAME.dl])
  m4_define([FACTS],[TESTDIR/facts])
  AT_CHECK(["$SOUFFLE" FLAGS -D. -F FACTS PROGRAM 0<TESTDIR/TESTNAME.in 1>TESTNAME.out 2>TESTNAME.err], [1])
  SAME_FILE([TESTNAME.out],[TESTDIR/TESTNAME.out])
  SAME_FILE([TESTNAME.err],[TESTDIR/TESTNAME.err])
])

dnl Positive testcase for Souffle
dnl $1 -- test name
dnl $2 -- category
m4_define([POSITIVE_TEST],[
  TEST_GROUP([$1],[
    TEST_EVAL([$1],[$2], facts)
  ])
])

dnl Positive testcase for Souffle
dnl $1 -- test name
dnl $2 -- category
dnl $3 -- facts directories
m4_define([POSITIVE_MULTI_TEST],[
  m4_foreach([POSITIVE_MULTI_TEST_DIR],[$3],[
    TEST_GROUP([$1 POSITIVE_MULTI_TEST_DIR],[
      TEST_EVAL([$1],[$2], POSITIVE_MULTI_TEST_DIR, /POSITIVE_MULTI_TEST_DIR)
    ])
  ])
])

dnl Negative testcase for Souffle
dnl $1 -- test name
dnl $2 -- category
m4_define([NEGATIVE_TEST],[
  TEST_GROUP([$1],[
    TEST_EVAL_ERROR([$1],[$2])
  ])
])

dnl Defines most relevant Souffle flag configurations for testing.
dnl NOTE: This is the default configuration that can be overridden
dnl using SOUFFLE_CONFS environment variable.
m4_define([DEFAULT_CONFS], [[],      dnl run default interpreter in sequential
  [-j8],                             dnl run interpreter in parallel
  [-c -j8],                          dnl compile, then execute in parallel
  [-c -j8 -efile]                    dnl compile, then execute in parallel with file communication engine
])

dnl Store user-defined souffle flag configuration given by the SOUFFLE_CONFS env (if any)
m4_define([ENV_CONFS],
  m4_split(m4_esyscmd_s(echo "[[$SOUFFLE_CONFS]]"),[,]))

dnl Pick a flag configuration for testing giving preference to user-defined flags
m4_ifblank(m4_join([],ENV_CONFS), [
  m4_define([CONFS], [DEFAULT_CONFS])
], [
  m4_define([CONFS], [ENV_CONFS])
])

m4_define([DEFAULT_CATEGORIES], [[Syntactic],
  [Semantic],
  [Evaluation],
  [Interface],
  [Profile],
  [Provenance],
])

dnl Store user-defined souffle flag configuration given by the SOUFFLE_CONFS env (if any)
m4_define([ENV_CATEGORIES],
  m4_split(m4_esyscmd_s(echo "[[$SOUFFLE_CATEGORY]]"),[,]))

dnl Set default values if none were given.
m4_ifblank(m4_join([],ENV_CATEGORIES), [
  m4_define([CATEGORIES], [DEFAULT_CATEGORIES])
], [
  m4_define([CATEGORIES], [ENV_CATEGORIES])
])

m4_foreach(current, [CATEGORIES],
[
  AT_BANNER([current])
  m4_if(current, Syntactic, [
    m4_include([syntactic.at])
  ])

  m4_if(current, Semantic, [
    m4_include([semantic.at])
  ])

  m4_if(current, Evaluation, [
    m4_include([evaluation.at])
  ])

  m4_if(current, FastEvaluation, [
    m4_include([fastevaluation.at])
  ])

  m4_if(current, Interface, [
    m4_include([interface.at])
  ])

  m4_if(current, Profile, [
    m4_include([profile.at])
  ])

  m4_if(current, Provenance, [
    m4_include([provenance.at])
  ])

  m4_if(current, Example, [
    m4_include([example.at])
  ])
])
